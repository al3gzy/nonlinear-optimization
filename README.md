# Nonlinear Optimization 

## ğŸ“ Project Structure

- `I` â€“ **Unconstrained Optimization**: Analysis of stationary points, optimization methods and visualization.
  
- `II` â€“ **Constrained Optimization**: Solving problems with constraints using KKT conditions and the projected gradient method.
  
- `III` â€“ **Nonlinear Least Squares**: Regression using Gauss-Newton and Levenberg-Marquardt methods.

## ğŸ§¾ Task Breakdown

### (I) Unconstrained Optimization

#### Functions:
- `fâ‚(x, y) = 1 / (1 + xÂ² + yÂ²)`
- `fâ‚‚(x, y) = (xÂ²yÂ²) / (1 + xÂ² + yÂ²)`
- `fâ‚ƒ(x, y) = (xy) / (xÂ² + yÂ² + 1)`

#### Goals:
- Find all stationary points and analyze the definiteness of the Hessian.
- Identify local minima, maxima and saddle points.
- Implement and compare optimization methods:
  - Gradient Descent
  - Newtonâ€™s Method
  - Inexact Newton Method
  - Quasi-Newton (BFGS)
- Plot:
  - Gradient norm vs iteration
  - 3D surface plots
  - Contour plots with method iteration paths

---

### (II) Constrained Optimization

#### Constraint Sets:
- `Sâ‚ = { (x, y) âˆˆ â„Â² : -1 â‰¤ x â‰¤ 1, -1 â‰¤ y â‰¤ 1 }`  (box constraint)
- `Sâ‚‚ = { (x, y) âˆˆ â„Â² : x + 2y = 0 }`  (linear equality constraint)

#### Goals:
- Solve the KKT system for each function and constraint set.
- Implement the Projected Gradient Method for box-constrained problems.
- Visualize feasible regions and optimization paths.

---

### (III) Nonlinear Least Squares

#### Model:
- `g(xâ‚, xâ‚‚; t) = xâ‚ * sin(xâ‚‚ * t)`

#### Data Points:
```text
(táµ¢, yáµ¢): (1,   0.8), (2,   1.5), (3,   0.9), (4,  -0.7), (5,  -1.9)
